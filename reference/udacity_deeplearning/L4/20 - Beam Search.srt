1
00:00:00,600 --> 00:00:02,420
RNN을 가지고 무엇을 할 수 있습니까?
What can you do with an?

2
00:00:02,420 --> 00:00:03,550
많은 것들이 있습니다.
Lots of things.

3
00:00:03,550 --> 00:00:05,500
sequence의 다음 단계를 예측하는
모델을 가지고 있다고 상상해보자.
Imagine that you have a model
that predicts the next step

4
00:00:05,500 --> 00:00:06,580
예를 들면,
of your sequence, for example.

5
00:00:06,580 --> 00:00:10,090
당신은 sequences을 생성할 수 있습니다.
You can use that to generate sequences.

6
00:00:10,090 --> 00:00:14,940
예를 들면, 텍스트, 한 시점에서의 단어 또는 
한 시점에서 문자를
For example, text, either one word at
a time or one character at a time.

7
00:00:14,940 --> 00:00:18,780
어느 시점(t)에서 sequence을 가지고 있으며,
RNN으로부터 예측을 생성할 수 있습니다.
For that, you take your sequence at
time, t, you generate the predictions

8
00:00:18,780 --> 00:00:22,120
예측된 분포로부터 샘플링을 할 수 있습니다.
from your RNN, and then you sample
from the predicted distribution.

9
00:00:22,120 --> 00:00:25,530
그것의 가능도를 기반으로 하나의 요소를 선택합니다.
And then you pick one element
based on its probability.

10
00:00:25,530 --> 00:00:28,830
그리고 다음 단계의 샘플을 선택하고 
계속해서 진행합니다.
And feed that sample to
the next step and go on.

11
00:00:28,830 --> 00:00:33,700
반복적으로 이것을 하고 예측, 샘플링, 예측, 샘플링 .. 
Do this repeatedly, predict,
sample, predict, sample, and

12
00:00:33,700 --> 00:00:37,580
RNN models의 무언가로부터 매우 좋은 
sequence을 생성할 수 있습니다.
you can generate a pretty good
sequence of whatever your RNN models.

13
00:00:37,580 --> 00:00:40,910
더나은 결과를 얻기 위해서 더욱 정교한 방법이 있습니다.
There is a more sophisticated way to
do this, that gives better results.

14
00:00:40,910 --> 00:00:45,300
모든 시점마다 다음 예측을 샘플링하기 때문에
이것은 매우 greedy(탐욕적)이다.
Because just sampling the next
prediction every time, is very greedy.

15
00:00:45,300 --> 00:00:47,730
각각의 단계에서 한번 샘플링 대신에
Instead of just sampling
once at each step,

16
00:00:47,730 --> 00:00:49,415
여러 시점에서 샘플링을 상상할 수 있습니다.
you could imagine
sampling multiple times.

17
00:00:49,415 --> 00:00:52,425
여기서는 예를 들면 O와 A 를 선택하는 것입니다.
Here we pick O but also A for example.

18
00:00:53,455 --> 00:00:56,795
우리는 여러 sequences을 가지고 있을때.
그 가정을 판정합니다.
When we have multiple sequences,
often call that hypotheses,

19
00:00:56,795 --> 00:00:59,915
모든 단계에서 예측을 계속할 수 있습니다.
that you could continue
predicting from at every step.

20
00:00:59,915 --> 00:01:03,175
이것들 중에서 가장 좋은 sequence을 선택할 수 있습니다.
You can choose the best sequence out
of those, by computing the total

21
00:01:03,175 --> 00:01:07,025
생성되어진 모든 문자들의 전체 확률을 선택하는 방법으로..
probability of all the characters
that you generated so far.

22
00:01:07,025 --> 00:01:10,800
어느 한 시점에 여러 시점의 단계에 전체 확률을 보면서.
By looking at the total probability
of the multiple time steps at a time.

23
00:01:10,800 --> 00:01:14,690
선택에 위해서 우연히 만들어지는 샘플링을 방지 할 수 있습니다.
You prevent the sampling from
accidentally making one by choice, and

24
00:01:14,690 --> 00:01:17,950
나쁜 결정에 의한 선택..
being stuck with that one
bad decision forever.

25
00:01:17,950 --> 00:01:20,950
예를 들면, 우연히 A를 선택하면,
For example, we could have just
picked A by chance here, and

26
00:01:20,950 --> 00:01:26,170
O라는 매우 합리적인 다음 단계로 이어지는 가정을 탐색하지 않습니다.
never explored the hypothesis that O
could lead to a very sensible next word.

27
00:01:26,170 --> 00:01:29,890
물론, 만약 그것을 하면 우리가 고려해야 할 sequences의
수는 기하급수적으로 증가합니다.
Of course, if you do this the number
of sequences that you need to consider

28
00:01:29,890 --> 00:01:30,950
grows exponentially.

29
00:01:32,000 --> 00:01:35,620
그것을 하는 좀더 스마트한 방법은,
Beam Search라고 불러지는 것이 있습니다.
There is a smarter way to do that, which
is to do what's called a Beam Search.

30
00:01:35,620 --> 00:01:39,760
모든 시점에서 가장 가능성있는 몇몇 후보 서열을 
단지 지키고, 말하는 것입니다.
You only keep, say, the most likely few
candidate sequences at every time step,

31
00:01:39,760 --> 00:01:41,600
그리고, 나머지는 단순히 제거하는 것입니다.
and then simply prune the rest.

32
00:01:41,600 --> 00:01:43,620
실제적으로, 매우 좋은 sequences을 생성하도록
매우 잘 동작합니다.
In practice, this works very well for

33
00:01:43,620 --> 00:01:45,610
generating very good
sequences from your model.

