1
00:00:00,320 --> 00:00:03,090
recurrent network의 파라메터
업데이트를 계산하기 위해서는
To compute the parameter
updates of a recurrent network,

2
00:00:03,090 --> 00:00:05,670
시간의 미분값을 backpropagate
할 필요가 있습니다.
we need to backpropagate
the derivative through time,

3
00:00:05,670 --> 00:00:07,700
sequence의 시작부분에서의 모든 방법은
all the way to the beginning
of the sequence.

4
00:00:07,700 --> 00:00:11,450
실제적으로는 더 자주, 우리가 
감당할 수 있을만큼의 많은 단계에서
Or in practice, more often, for
as many steps as we can afford.

5
00:00:11,450 --> 00:00:15,510
모든 미분값들은 같은 파라메터를
적용합니다.
All these derivatives are going to
be applied to the same parameters.

6
00:00:15,510 --> 00:00:19,950
한번에 업데이트되는 연관된 모든 값들은 
같은 가중치를 갖습니다.
That's a lot of correlated updates
all at once, for the same weights.

7
00:00:19,950 --> 00:00:22,315
Stochastic gradient descent에서는
나쁜 결과가 나옵니다.
This is bad for
stochastic gradient descent.

8
00:00:22,315 --> 00:00:25,560
Stochastic gradient descent은
parameters에 대해서 연관되지 않은 업데이트로 간주합니다.
Stochastic gradient descent
prefers to have uncorrelated updates,

9
00:00:25,560 --> 00:00:28,820
training의 안정성을 위해서..
to its parameters, for
the stability of the training.

10
00:00:28,820 --> 00:00:31,300
이것은 매우 수리적으로 매우 불안정합니다.
This makes the math very unstable.

11
00:00:31,300 --> 00:00:35,170
기울기가 지수적으로 증가해서
무한으로 발산하거나
Either the gradients grow exponentially
and you end up with infinities.

12
00:00:35,170 --> 00:00:37,550
기울기가 떨어져서 매우 빨리 zero로 수렴합니다.
Or they go down to zero very quickly.

13
00:00:37,550 --> 00:00:38,970
어느쪽도 training을 끝나지 못합니다.
And you end up not training anything.

